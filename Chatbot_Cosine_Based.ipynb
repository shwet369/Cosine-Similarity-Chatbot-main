{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "bca81529",
      "metadata": {
        "id": "bca81529"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"test.csv\", encoding='unicode_escape')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "81ddbe6f",
      "metadata": {
        "id": "81ddbe6f"
      },
      "outputs": [],
      "source": [
        "questions_list = df['Questions'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "64601d5e",
      "metadata": {
        "id": "64601d5e"
      },
      "outputs": [],
      "source": [
        "answers_list = df['Answers'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "a3bffd3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3bffd3a",
        "outputId": "2836429f-1ca8-41ea-cba0-0f93ac545368"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to C:\\Users\\SHWETA\n",
            "[nltk_data]     BHOYAR\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to C:\\Users\\SHWETA\n",
            "[nltk_data]     BHOYAR\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to C:\\Users\\SHWETA\n",
            "[nltk_data]     BHOYAR\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "5e900a08",
      "metadata": {
        "id": "5e900a08"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "def preprocess(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stemmer = PorterStemmer()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove non-alphanumeric characters\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens]\n",
        "    return ' '.join(stemmed_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "194886bb",
      "metadata": {
        "id": "194886bb"
      },
      "outputs": [],
      "source": [
        "def preprocess_with_stopwords(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stemmer = PorterStemmer()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove non-alphanumeric characters\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens]\n",
        "    return ' '.join(stemmed_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "d4d658f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4d658f6",
        "outputId": "6292ea33-aa38-462a-9a4b-f4e19ef42135"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize)\n",
        "X = vectorizer.fit_transform([preprocess(q) for q in questions_list])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "a0320d12",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "a0320d12",
        "outputId": "13ba0331-5492-4b86-ea40-74bcb93ed502"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processed_text: who is m dhoni\n",
            "similarities: [[0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "max_similarity: 0.0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"I can't answer this question.\""
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize)\n",
        "X = vectorizer.fit_transform([preprocess(q) for q in questions_list])\n",
        "\n",
        "def get_response(text):\n",
        "    processed_text = preprocess_with_stopwords(text)\n",
        "    print(\"processed_text:\", processed_text)\n",
        "    vectorized_text = vectorizer.transform([processed_text])\n",
        "    similarities = cosine_similarity(vectorized_text, X)\n",
        "    print(\"similarities:\", similarities)\n",
        "    max_similarity = np.max(similarities)\n",
        "    print(\"max_similarity:\", max_similarity)\n",
        "    if max_similarity > 0.6:\n",
        "        high_similarity_questions = [q for q, s in zip(questions_list, similarities[0]) if s > 0.6]\n",
        "        print(\"high_similarity_questions:\", high_similarity_questions)\n",
        "\n",
        "        target_answers = []\n",
        "        for q in high_similarity_questions:\n",
        "            q_index = questions_list.index(q)\n",
        "            target_answers.append(answers_list[q_index])\n",
        "        print(target_answers)\n",
        "\n",
        "        Z = vectorizer.fit_transform([preprocess_with_stopwords(q) for q in high_similarity_questions])\n",
        "        processed_text_with_stopwords = preprocess_with_stopwords(text)\n",
        "        print(\"processed_text_with_stopwords:\", processed_text_with_stopwords)\n",
        "        vectorized_text_with_stopwords = vectorizer.transform([processed_text_with_stopwords])\n",
        "        final_similarities = cosine_similarity(vectorized_text_with_stopwords, Z)\n",
        "        closest = np.argmax(final_similarities)\n",
        "        return target_answers[closest]\n",
        "    else:\n",
        "        return \"I can't answer this question.\"\n",
        "\n",
        "get_response('Who is ms dhoni?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "NUoJPaJUX9co",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "NUoJPaJUX9co",
        "outputId": "665663eb-e438-446f-c343-909d76721f89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processed_text: what is machin learn\n",
            "similarities: [[0.         0.         0.         0.         0.77627227 0.\n",
            "  0.         0.        ]]\n",
            "max_similarity: 0.7762722680124386\n",
            "high_similarity_questions: ['What is the role of machine learning in data analytics?']\n",
            "['Machine learning plays a crucial role in data analytics by enabling the development of algorithms that can automatically learn from data and make predictions or take actions without being explicitly programmed. It is used for tasks such as classification, regression, clustering, and anomaly detection.']\n",
            "processed_text_with_stopwords: what is machin learn\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Machine learning plays a crucial role in data analytics by enabling the development of algorithms that can automatically learn from data and make predictions or take actions without being explicitly programmed. It is used for tasks such as classification, regression, clustering, and anomaly detection.'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize)\n",
        "X = vectorizer.fit_transform([preprocess(q) for q in questions_list])\n",
        "\n",
        "def get_response(text):\n",
        "    processed_text = preprocess_with_stopwords(text)\n",
        "    print(\"processed_text:\", processed_text)\n",
        "    vectorized_text = vectorizer.transform([processed_text])\n",
        "    similarities = cosine_similarity(vectorized_text, X)\n",
        "    print(\"similarities:\", similarities)\n",
        "    max_similarity = np.max(similarities)\n",
        "    print(\"max_similarity:\", max_similarity)\n",
        "    if max_similarity > 0.6:\n",
        "        high_similarity_questions = [q for q, s in zip(questions_list, similarities[0]) if s > 0.6]\n",
        "        print(\"high_similarity_questions:\", high_similarity_questions)\n",
        "\n",
        "        target_answers = []\n",
        "        for q in high_similarity_questions:\n",
        "            q_index = questions_list.index(q)\n",
        "            target_answers.append(answers_list[q_index])\n",
        "        print(target_answers)\n",
        "\n",
        "        Z = vectorizer.fit_transform([preprocess_with_stopwords(q) for q in high_similarity_questions])\n",
        "        processed_text_with_stopwords = preprocess_with_stopwords(text)\n",
        "        print(\"processed_text_with_stopwords:\", processed_text_with_stopwords)\n",
        "        vectorized_text_with_stopwords = vectorizer.transform([processed_text_with_stopwords])\n",
        "        final_similarities = cosine_similarity(vectorized_text_with_stopwords, Z)\n",
        "        closest = np.argmax(final_similarities)\n",
        "        return target_answers[closest]\n",
        "    else:\n",
        "        return \"I can't answer this question.\"\n",
        "\n",
        "get_response('what is machine learning')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "YHHBV_uDhrSB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHHBV_uDhrSB",
        "outputId": "0ce83955-94ea-49e0-e7fc-4dfd215bb010"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is Data Analytics\n"
          ]
        }
      ],
      "source": [
        "import language_tool_python\n",
        "\n",
        "# Use the remote server\n",
        "tool = language_tool_python.LanguageToolPublicAPI('en-US')\n",
        "\n",
        "text = \"What is Data Anlytics\"\n",
        "matches = tool.check(text)\n",
        "corrected_text = tool.correct(text)\n",
        "\n",
        "print(corrected_text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "uJ_s0VrsjGcs",
      "metadata": {
        "id": "uJ_s0VrsjGcs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\ProgramData\\anaconda3\\python.exe\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0a36a67",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
